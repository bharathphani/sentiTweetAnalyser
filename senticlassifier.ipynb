{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rte to /Users/bhogirala/nltk_data...\n",
      "[nltk_data]   Package rte is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bhogirala/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sys, os\n",
    "import  re\n",
    "import random\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('rte')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtil :\n",
    "\n",
    "    def getTrainingAndTestData(self, tweets, K, k):\n",
    "\n",
    "        from functools import wraps\n",
    "        \n",
    "        procTweets = tweets[:10]\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "        all_tweets = []  # DATADICT: all_tweets =   [ (words, sentiment), ... ]\n",
    "        for tuple in procTweets.itertuples():\n",
    "            \n",
    "            words = [word if (word[0:2] == '__') else word.lower() \\\n",
    "                     for word in tuple[2].split() \\\n",
    "                     if len(word) >= 3]\n",
    "            words = [stemmer.stem(w) for w in words]  # DATADICT: words = [ 'word1', 'word2', ... ]\n",
    "            all_tweets.append((words, tuple[1]))\n",
    "\n",
    "        train_tweets = [x for i, x in enumerate(all_tweets) if i % K != k]\n",
    "        test_tweets = [x for i, x in enumerate(all_tweets) if i % K == k]\n",
    "\n",
    "\n",
    "        def get_word_features(words):\n",
    "            bag = {}\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            filtered_words = [w for w in words if not w in stop_words]\n",
    "            words_uni = ['has(%s)' % ug for ug in filtered_words]\n",
    "            for f in words_uni:\n",
    "                bag[f] = 1\n",
    "\n",
    "            # bag = collections.Counter(words_uni+words_bi+words_tri)\n",
    "            return bag\n",
    "\n",
    "        negtn_regex = re.compile(r\"\"\"(?:\n",
    "            ^(?:never|no|nothing|nowhere|noone|none|not|\n",
    "                havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "                wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint\n",
    "            )$\n",
    "        )\n",
    "        |\n",
    "        n't\n",
    "        \"\"\", re.X)\n",
    "\n",
    "        pos_regex = re.compile(r\"\"\"(?:\n",
    "                    ^(?:excellent|wow|awesome|happy|cool|good|love|\n",
    "                        wonderful|amazing|amaze|bliss|enjoy|fantastic|\n",
    "                        beautiful|beauty|better|doesnt|fun|funny|arent|luck|lucky|\n",
    "                        nice|super\n",
    "                    )$\n",
    "                )\n",
    "                |\n",
    "                n't\n",
    "                \"\"\", re.X)\n",
    "\n",
    "        def get_negation_features(words):\n",
    "            INF = 0.0\n",
    "            negtn = [bool(negtn_regex.search(w)) for w in words]\n",
    "\n",
    "            left = [0.0] * len(words)\n",
    "            prev = 0.0\n",
    "            for i in range(0, len(words)):\n",
    "                if (negtn[i]):\n",
    "                    prev = 1.0\n",
    "                left[i] = prev\n",
    "                prev = max(0.0, prev - 0.1)\n",
    "\n",
    "            right = [0.0] * len(words)\n",
    "            prev = 0.0\n",
    "            for i in reversed(range(0, len(words))):\n",
    "                if (negtn[i]):\n",
    "                    prev = 1.0\n",
    "                right[i] = prev\n",
    "                prev = max(0.0, prev - 0.1)\n",
    "\n",
    "            return dict(zip(\n",
    "                ['neg_l(' + w + ')' for w in words] + ['neg_r(' + w + ')' for w in words],\n",
    "                left + right))\n",
    "\n",
    "        def get_positive_features(words):\n",
    "\n",
    "            bag={}\n",
    "            for word in words:\n",
    "                if bool(pos_regex.search(word)):\n",
    "                    key = 'pos(' + word + ')'\n",
    "                    bag[key] = 1\n",
    "            return bag\n",
    "\n",
    "\n",
    "        def counter(func):  \n",
    "            @wraps(func)\n",
    "            def tmp(*args, **kwargs):\n",
    "                tmp.count += 1\n",
    "                return func(*args, **kwargs)\n",
    "\n",
    "            tmp.count = 0\n",
    "            return tmp\n",
    "\n",
    "        @counter  \n",
    "        def extract_features(words):\n",
    "\n",
    "            features = {}\n",
    "            negation_features = get_negation_features(words)\n",
    "            features.update(negation_features)\n",
    "            postive_features = get_positive_features(words)\n",
    "            features.update(postive_features)\n",
    "            word_features = get_word_features(words)\n",
    "            features.update(word_features)\n",
    "            sys.stderr.write('\\rfeatures extracted for ' + str(extract_features.count) + ' tweets')\n",
    "            return features\n",
    "\n",
    "        extract_features.count = 0;\n",
    "        tweets_processed = 0\n",
    "        # Apply NLTK's Lazy Map\n",
    "        print(\"length of train tweets \"+str(len(train_tweets)))\n",
    "        v_train = nltk.classify.apply_features(extract_features, train_tweets)\n",
    "        print(\"length of test tweets \" + str(len(test_tweets)))\n",
    "        v_test = nltk.classify.apply_features(extract_features, test_tweets)\n",
    "        return (v_train, v_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        print (\"Sentiment Classifier Started\")\n",
    "\n",
    "    def get_time_stamp(self):\n",
    "        return time.strftime(\"%y%m%d-%H%M%S-%Z\")\n",
    "\n",
    "    def grid(self, alist, blist):\n",
    "        for a in alist:\n",
    "            for b in blist:\n",
    "                yield (a, b)\n",
    "\n",
    "\n",
    "    NUM_SHOW_FEATURES = 100\n",
    "    SPLIT_RATIO = 0.9\n",
    "    FOLDS = 5\n",
    "    LIST_CLASSIFIERS = ['NaiveBayesClassifier', 'MaxentClassifier','SvmClassifier','DecisiontreeClassifier','RTEClassifier']\n",
    "\n",
    "    def trainAndClassify(self, tweets, classifier, fileprefix):\n",
    "\n",
    "        dataUtil  = DataUtil()\n",
    "        INFO = str(classifier)\n",
    "        if (len(fileprefix) > 0 and '_' != fileprefix[0]):\n",
    "            directory = os.path.dirname(fileprefix)\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            realstdout = sys.stdout\n",
    "            sys.stdout = open(fileprefix + '_' + INFO + '.txt', 'w')\n",
    "        print (INFO)\n",
    "        sys.stderr.write('\\n' + '#' * 80 + '\\n' + INFO)\n",
    "        if ('NaiveBayesClassifier' == classifier):\n",
    "            CLASSIFIER = nltk.classify.NaiveBayesClassifier\n",
    "\n",
    "            def train_function(v_train):\n",
    "                return CLASSIFIER.train(v_train)\n",
    "        elif ('MaxentClassifier' == classifier):\n",
    "            CLASSIFIER = nltk.classify.MaxentClassifier\n",
    "\n",
    "            def train_function(v_train):\n",
    "                return CLASSIFIER.train(v_train, algorithm='GIS', max_iter=10)\n",
    "        elif ('SvmClassifier' == classifier):\n",
    "            CLASSIFIER = nltk.classify.scikitlearn.SklearnClassifier(SVC())\n",
    "\n",
    "            def SvmClassifier_show_most_informative_features(self, n=10):\n",
    "                print ('unimplemented')\n",
    "\n",
    "            CLASSIFIER.show_most_informative_features = SvmClassifier_show_most_informative_features\n",
    "\n",
    "            def train_function(v_train):\n",
    "                return CLASSIFIER.train(v_train)\n",
    "        \n",
    "\n",
    "        elif('DecisiontreeClassifier' == classifier):\n",
    "            CLASSIFIER = nltk.classify.DecisionTreeClassifier\n",
    "            \n",
    "            def DecisionTreeClassifier_show_most_informative_features( self, n=10 ):\n",
    "                print ('unimplemented')\n",
    "                    \n",
    "        \n",
    "            \n",
    "            CLASSIFIER.show_most_informative_features = DecisionTreeClassifier_show_most_informative_features\n",
    "                \n",
    "            def train_function(v_train):\n",
    "                    return CLASSIFIER.train(v_train)\n",
    "\n",
    "        elif ('RTEClassifier' == classifier):\n",
    "            CLASSIFIER = nltk.classify.rte_classifier('IIS')\n",
    "\n",
    "            def train_function(v_train):\n",
    "                return CLASSIFIER.train(v_train)\n",
    "\n",
    "        accuracies = []\n",
    "        \n",
    "        for k in range(self.FOLDS):\n",
    "                (v_train, v_test) = dataUtil.getTrainingAndTestData(tweets, self.FOLDS, k)\n",
    "\n",
    "                sys.stderr.write('\\n[training start]')\n",
    "                classifier_tot = train_function(v_train)\n",
    "                sys.stderr.write(' [training complete]')\n",
    "\n",
    "                print ('######################')\n",
    "                print ('1 Step Classifier : ', classifier)\n",
    "                accuracy_tot = nltk.classify.accuracy(classifier_tot, v_test)\n",
    "                print ('Accuracy : ', accuracy_tot)\n",
    "                print ('######################')\n",
    "                print (classifier_tot.show_most_informative_features(self.NUM_SHOW_FEATURES))\n",
    "                print ('######################')\n",
    "\n",
    "                # build confusion matrix over test set\n",
    "                test_truth = [s for (t, s) in v_test]\n",
    "                test_predict = [classifier_tot.classify(t) for (t, s) in v_test]\n",
    "\n",
    "                print ('Accuracy :', accuracy_tot)\n",
    "                print ('Confusion Matrix ')\n",
    "                print (nltk.ConfusionMatrix(test_truth, test_predict))\n",
    "\n",
    "                accuracies.append(accuracy_tot)\n",
    "        print (\"Accuracies:\", accuracies)\n",
    "        print (\"Average Accuracy:\", sum(accuracies) / self.FOLDS)\n",
    "\n",
    "\n",
    "        sys.stderr.write('\\nAccuracies :')\n",
    "        for k in range(self.FOLDS):\n",
    "            sys.stderr.write(' %0.5f' % accuracies[k])\n",
    "        sys.stderr.write('\\nAverage Accuracy: %0.5f\\n' % (sum(accuracies) / self.FOLDS))\n",
    "        sys.stderr.flush()\n",
    "\n",
    "        sys.stdout.flush()\n",
    "        if (len(fileprefix) > 0 and '_' != fileprefix[0]):\n",
    "            sys.stdout.close()\n",
    "            sys.stdout = realstdout\n",
    "\n",
    "        return classifier_tot\n",
    "\n",
    "    def main(self):\n",
    "        \n",
    "        fileprefix = 'logs/run'\n",
    "        tweets = pd.read_csv(\"./data/preprocessed_tweets_shuffled.csv\",encoding='ISO-8859-1',names=[\"label\", \"id\", \"date\", \"query\", \"user\", \"tweet\"])\n",
    "        sys.stderr.write('\\nlen( tweets ) = ' + str(len(tweets)))\n",
    "        TIME_STAMP = self.get_time_stamp()\n",
    "        for cname in self.LIST_CLASSIFIERS:\n",
    "\n",
    "                self.trainAndClassify(\n",
    "                    tweets, classifier=cname,fileprefix=fileprefix + '_' + TIME_STAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Classifier Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3191: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n",
      "\n",
      "len( tweets ) = 1600001\n",
      "################################################################################\n",
      "NaiveBayesClassifier\n",
      "features extracted for 16 tweets[training complete]\n",
      "features extracted for 16 tweets[training complete]\n",
      "features extracted for 16 tweets[training complete]\n",
      "features extracted for 16 tweets[training complete]\n",
      "features extracted for 16 tweets[training complete]\n",
      "Accuracies : 0.50000 0.00000 1.00000 0.50000 0.50000\n",
      "Average Accuracy: 0.50000\n",
      "\n",
      "################################################################################\n",
      "MaxentClassifier\n",
      "features extracted for 544 tweets [training complete]\n",
      "features extracted for 544 tweets [training complete]\n",
      "features extracted for 544 tweets [training complete]\n",
      "features extracted for 544 tweets [training complete]\n",
      "features extracted for 544 tweets [training complete]\n",
      "Accuracies : 0.50000 0.00000 0.50000 0.50000 0.00000\n",
      "Average Accuracy: 0.30000\n",
      "\n",
      "################################################################################\n",
      "SvmClassifier\n",
      "features extracted for 8 tweets/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "features extracted for 16 tweets\n",
      "features extracted for 16 tweets[training complete]\n",
      "features extracted for 16 tweets[training complete]\n",
      "features extracted for 16 tweets[training complete]\n",
      "features extracted for 16 tweets[training complete]\n",
      "Accuracies : 0.50000 0.50000 1.00000 1.00000 0.00000\n",
      "Average Accuracy: 0.60000\n",
      "\n",
      "################################################################################\n",
      "DecisiontreeClassifier\n",
      "features extracted for 4016 tweets [training complete]\n",
      "features extracted for 4040 tweets [training complete]\n",
      "features extracted for 4760 tweets [training complete]\n",
      "features extracted for 4088 tweets [training complete]\n",
      "features extracted for 4400 tweets [training complete]\n",
      "Accuracies : 0.50000 0.00000 0.50000 0.50000 0.00000\n",
      "Average Accuracy: 0.30000\n",
      "\n",
      "################################################################################\n",
      "RTEClassifier\n",
      "features extracted for 5592 tweets [training complete]\n",
      "features extracted for 5592 tweets [training complete]\n",
      "features extracted for 5592 tweets [training complete]\n",
      "features extracted for 5592 tweets [training complete]\n",
      "features extracted for 5592 tweets [training complete]\n",
      "Accuracies : 0.50000 0.00000 0.50000 0.50000 0.00000\n",
      "Average Accuracy: 0.30000\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier()\n",
    "classifier.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
